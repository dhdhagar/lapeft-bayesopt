{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694e282e-3d89-427e-9a4c-7974972c8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a59553-0986-4165-b965-6148716551c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bc36a0-11cc-4032-b147-cb6e0bb6be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d40d547-6f24-44ce-a970-e3a578ffac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.053352118"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.similarity('twitter', 'apple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608d28f8-da08-452f-88f5-8aba7a9b5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2afc0231-16ab-42a7-8170-c645bfd6ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../data/twentyquestions/datasets/word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e93a63-afd2-4642-af94-dcb730ff0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a18da2-9a72-44b1-bf29-5a6daba9748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.read_csv('../data/twentyquestions/dev-1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6e5548-0ce4-47be-8973-b283997dc0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>warhorse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crossfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>anchorage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>plank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>thermograph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>chloroform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>participant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>bakery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Words\n",
       "0          table\n",
       "1       warhorse\n",
       "2          gland\n",
       "3      crossfire\n",
       "4      anchorage\n",
       "..           ...\n",
       "995        plank\n",
       "996  thermograph\n",
       "997   chloroform\n",
       "998  participant\n",
       "999       bakery\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f2dc6ba-7b44-429a-a4d7-f21cc5eb209d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_list = words['Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e36fbdbb-cd0d-4801-a0d0-9557b4aa6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'computer'\n",
    "similarities = []\n",
    "not_in_index = []\n",
    "for w in word_list:\n",
    "    try:\n",
    "        sim = vectors.similarity(test, w)\n",
    "    except KeyError:\n",
    "        not_in_index.append(w)\n",
    "        similarities.append(0.)\n",
    "        continue\n",
    "    similarities.append(sim)\n",
    "similarities = [(sim - min(similarities)) / (max(similarities) - min(similarities)) for sim in similarities]\n",
    "tups = list(zip(word_list, similarities))\n",
    "for i,t in enumerate(tups):\n",
    "    if t[0] in not_in_index:\n",
    "        tups[i] = (t[0], 0.)\n",
    "tups.sort(key=lambda x: -x[1])\n",
    "word_dataset = [{'Words': t[0], 'Similarity': t[1]} for t in tups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a582010-cf9d-4807-b265-951d38b38910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(word_dataset).to_csv(f'../data/twentyquestions/datasets/word2vec/{test}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7099578-f315-456a-a4d7-ed7fc1fa3e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcb0c536-44ce-49a4-a465-71e1c67437cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'papa', 'trees', 'child', 'sax', 'crane', 'meatloaf', 'birthstone', 'polyethylene', 'cement']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "test_words = random.choices(list(set(word_list)-set(not_in_index+['computer'])), k=9)\n",
    "test_words = ['computer'] + test_words\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c7cc550-7238-4940-a039-a8cda3f07eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in test_words:\n",
    "    similarities = []\n",
    "    not_in_index = []\n",
    "    # Compute cosine sim\n",
    "    for w in word_list:\n",
    "        try:\n",
    "            sim = vectors.similarity(test, w)\n",
    "        except KeyError:\n",
    "            not_in_index.append(w)\n",
    "            similarities.append(0.)\n",
    "            continue\n",
    "        similarities.append(sim)\n",
    "    # 0-1 normalization\n",
    "    similarities = [(sim - min(similarities)) / (max(similarities) - min(similarities)) for sim in similarities]\n",
    "    tups = list(zip(word_list, similarities))\n",
    "    # Set not_in_index words to 0.\n",
    "    for i,t in enumerate(tups):\n",
    "        if t[0] in not_in_index:\n",
    "            tups[i] = (t[0], 0.)\n",
    "    # Sort list in descending order of similarity\n",
    "    tups.sort(key=lambda x: -x[1])\n",
    "    word_dataset = [{'Words': t[0], 'Similarity': t[1]} for t in tups]\n",
    "    # Save dataset as a csv\n",
    "    pd.DataFrame(word_dataset).to_csv(f'../data/twentyquestions/datasets/word2vec/{test}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a79536-dcad-4cb4-b8dc-3ca7b0b46db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
