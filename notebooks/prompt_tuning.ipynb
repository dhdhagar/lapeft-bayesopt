{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695333fe-b2e6-40ac-b03a-8b44c2b540a2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8e5d032-6bb3-47a4-b3a7-829d04ceda98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/dagarwal_umass_edu/.conda/envs/lapeft/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5EncoderModel, T5ForConditionalGeneration, T5Config, AutoModelForCausalLM, \\\n",
    "    AutoTokenizer, T5Tokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, DataCollatorWithPadding, get_scheduler, \\\n",
    "    EarlyStoppingCallback, IntervalStrategy, TrainerCallback, LlamaConfig\n",
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit, PeftModel\n",
    "import schedulefree\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from IPython import embed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103f4fc9-1bac-4d84-8a2f-377e44e1b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"./\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d343cc-cfbf-4f65-9828-74e66e636efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedAdamW(torch.optim.AdamW):\n",
    "    \"\"\"\n",
    "    A variant of Adam where some of the parameters are constrained to have unit norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, constrained_params, lr, scale_factor=1., weight_decay=0.01):\n",
    "        super().__init__(params, lr=lr, weight_decay=weight_decay)\n",
    "        self.constrained_params = list(constrained_params)\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        with torch.no_grad():\n",
    "            for p in self.constrained_params:\n",
    "                normed_p = p / p.norm() * self.scale_factor  # p.norm(dim=0, keepdim=True)\n",
    "                # project away the parallel component of the gradient\n",
    "                p.grad -= (p.grad * normed_p).sum(dim=-1, keepdim=False) * normed_p  # (dim=0, keepdim=True)\n",
    "        super().step(closure=closure)\n",
    "        with torch.no_grad():\n",
    "            for p in self.constrained_params:\n",
    "                # renormalize the constrained parameters\n",
    "                p /= p.norm() * self.scale_factor  # p.norm(dim=0, keepdim=True)\n",
    "\n",
    "class ConstrainedAdamWScheduleFree(schedulefree.AdamWScheduleFree):\n",
    "    \"\"\"\n",
    "    A variant of Adam where some of the parameters are constrained to have unit norm.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, constrained_params, lr, warmup_steps=100):\n",
    "        super().__init__(params, lr=lr, warmup_steps=warmup_steps)\n",
    "        self.constrained_params = list(constrained_params)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        with torch.no_grad():\n",
    "            for p in self.constrained_params:\n",
    "                normed_p = p / p.norm() # p.norm(dim=0, keepdim=True)\n",
    "                # project away the parallel component of the gradient\n",
    "                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p\n",
    "        super().step(closure=closure)\n",
    "        with torch.no_grad():\n",
    "            for p in self.constrained_params:\n",
    "                # renormalize the constrained parameters\n",
    "                p /= p.norm() # p.norm(dim=0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279ea3c6-8dd7-441f-a33d-ae489fd8c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenizer, shuffle_input=False, constant_input=None, seed=17):\n",
    "    # Load dataset\n",
    "    raw_dataset = [{'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. \\\n",
    "I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type \\\n",
    "commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. \\\n",
    "my first command is pwd'},\n",
    "              {'prompt': 'I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. \\\n",
    "I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type \\\n",
    "commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. \\\n",
    "my first command is chmod',\n",
    "               }]\n",
    "    hf_dataset = datasets.Dataset.from_pandas(pd.DataFrame(data=raw_dataset))\n",
    "    if not MODEL_NAME.startswith('t5'):\n",
    "        hf_dataset = hf_dataset.map(lambda x: tokenizer(\" \".join(x[\"prompt\"].split()[:-1]), text_target=x[\"prompt\"].split()[-1]))\n",
    "        if shuffle_input:\n",
    "            random.seed(seed)\n",
    "            hf_dataset = hf_dataset.map(lambda x: {**x, \"input_ids\": random.sample(x[\"input_ids\"], len(x[\"input_ids\"]))})\n",
    "        if constant_input is not None:\n",
    "            hf_dataset = hf_dataset.map(lambda x: {**x, \"input_ids\": [constant_input]*len(x[\"input_ids\"])})\n",
    "        hf_dataset = hf_dataset.map(lambda x: {**x, \"input_ids\": x[\"input_ids\"] + x[\"labels\"] + [tokenizer.eos_token_id],\n",
    "                                               \"attention_mask\": x[\"attention_mask\"] + [1]*len(x[\"labels\"]) + [1],\n",
    "                                               \"labels\": [-100]*len(x[\"input_ids\"]) + x[\"labels\"] + [tokenizer.eos_token_id]})\n",
    "        # hf_dataset = hf_dataset.map(lambda x: tokenizer(x[\"prompt\"]))\n",
    "        # hf_dataset = hf_dataset.map(lambda x: {**x, \"input_ids\": x[\"input_ids\"]+[tokenizer.eos_token_id], \"attention_mask\": x[\"attention_mask\"]+[1]})\n",
    "    else:\n",
    "        hf_dataset = hf_dataset.map(lambda x: tokenizer(\" \".join(x[\"prompt\"].split()[:-1]), text_target=x[\"prompt\"].split()[-1]))\n",
    "        # hf_dataset = hf_dataset.map(lambda x: tokenizer(\"Instruction\", text_target=x[\"prompt\"]))\n",
    "        # hf_dataset = hf_dataset.map(lambda x: {**x, \"attention_mask\": [0]*len(x[\"attention_mask\"])})  # Don't attend to the encoder input tokens (only attend to the encoder virtual token)\n",
    "    return hf_dataset\n",
    "\n",
    "\n",
    "def get_outputs(model, inputs=None, inputs_embeds=None, decoder_inputs_embeds=None, max_new_tokens=500, device='cuda', text=True,\n",
    "                greedy=False):\n",
    "    decoding_args = {}\n",
    "    if not greedy:\n",
    "        decoding_args = {\n",
    "            \"temperature\": 0.5,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": True,\n",
    "            \"repetition_penalty\": 1.5, #Avoid repetition.\n",
    "            \"early_stopping\": True, #The model can stop before reach the max_length\n",
    "        }\n",
    "    if inputs_embeds is not None or decoder_inputs_embeds is not None:\n",
    "        outputs = model.generate(\n",
    "            inputs_embeds=None if inputs_embeds is None else inputs_embeds.to(device),\n",
    "            decoder_inputs_embeds=None if decoder_inputs_embeds is None else decoder_inputs_embeds.to(device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **decoding_args\n",
    "        )    \n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            **decoding_args\n",
    "        )\n",
    "    if text:\n",
    "        return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6, device='cuda'):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        use_cpu=device=='cpu', # This is necessary for CPU clusters.\n",
    "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate=learning_rate, # Higher learning rate than full Fine-Tuning\n",
    "        num_train_epochs=epochs,\n",
    "        logging_steps=epochs//10,\n",
    "        eval_steps=epochs//10,\n",
    "        metric_for_best_model='accuracy', # 'loss',\n",
    "        load_best_model_at_end = True,\n",
    "        save_strategy=IntervalStrategy.STEPS,\n",
    "        evaluation_strategy=IntervalStrategy.STEPS\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "class CustomEarlyStoppingCallback(EarlyStoppingCallback):\n",
    "    def __init__(self):\n",
    "        super()\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        is_correct = kwargs['metrics']['eval_accuracy']\n",
    "        if is_correct:\n",
    "            control.should_training_stop = True\n",
    "\n",
    "def create_trainer(peft_model, training_args, train_dataset, schedule_free=False,\n",
    "                   unit_norm=False, unit_norm_scale_factor=1.):\n",
    "    add_args = {}\n",
    "    if schedule_free:\n",
    "        if unit_norm:\n",
    "            optimizer = ConstrainedAdamWScheduleFree(\n",
    "                params=peft_model.parameters(),\n",
    "                constrained_params=peft_model.prompt_encoder.parameters(),\n",
    "                lr=training_args.learning_rate,\n",
    "                warmup_steps=100\n",
    "            )\n",
    "        else:\n",
    "            optimizer = schedulefree.AdamWScheduleFree(\n",
    "                peft_model.parameters(),\n",
    "                lr=training_args.learning_rate,\n",
    "                warmup_steps=100,\n",
    "            )\n",
    "        add_args[\"optimizers\"] = (optimizer, None)\n",
    "    elif unit_norm:\n",
    "        optimizer = ConstrainedAdamW(\n",
    "            params=peft_model.parameters(),\n",
    "            constrained_params=peft_model.prompt_encoder.parameters(),\n",
    "            lr=training_args.learning_rate,\n",
    "            scale_factor=unit_norm_scale_factor)\n",
    "        add_args[\"optimizers\"] = (optimizer, None)\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=peft_model) if MODEL_NAME.startswith('t5') else DataCollatorWithPadding(tokenizer)  # DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        _type = \"seq2seq\" if type(eval_pred.predictions) is tuple else \"causal\"\n",
    "        preds = eval_pred.predictions[0] if _type == \"seq2seq\" else eval_pred.predictions\n",
    "        preds = preds.argmax(axis=-1).squeeze()  # greedy\n",
    "        labels = eval_pred.label_ids.squeeze()\n",
    "        is_correct = labels[labels != -100] == preds[:len(preds) if _type == \"seq2seq\" else (len(preds) - 1)][labels != -100]\n",
    "        return {'accuracy': (is_correct.sum() / len(is_correct)) == 1}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[CustomEarlyStoppingCallback()], #[EarlyStoppingCallback(early_stopping_patience=1)],  # , early_stopping_threshold=0.2\n",
    "        compute_metrics=compute_metrics,\n",
    "        **add_args\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def load_and_set_adapter(directory, name):\n",
    "    loaded_model.load_adapter(directory, adapter_name=name)\n",
    "    loaded_model.set_adapter(name)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def get_virtual_token(foundational_model, tokenizer, hf_dataset, data_idx=0, num_virtual_tokens=1, learning_rate=3e-3, \n",
    "                      epochs=500, save=True, reset=False, schedule_free=False, unit_norm=False, unit_norm_scale_factor=1.):\n",
    "    output_directory =  os.path.join(working_dir, f\"peft_model_{data_idx}\")\n",
    "    # Check if the model already exists\n",
    "    if not reset and os.path.exists(output_directory):\n",
    "        peft_model = PeftModel.from_pretrained(foundational_model,\n",
    "                                               output_directory,\n",
    "                                               device_map='auto',\n",
    "                                               is_trainable=False)\n",
    "    else:\n",
    "        # Load peft model\n",
    "        generation_config = PromptTuningConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM if MODEL_NAME.startswith('t5') else TaskType.CAUSAL_LM,\n",
    "            prompt_tuning_init=PromptTuningInit.RANDOM,  # PromptTuningInit.RANDOM if MODEL_NAME.startswith('t5') else PromptTuningInit.TEXT,  # PromptTuningInit.RANDOM,\n",
    "            prompt_tuning_init_text=tokenizer.decode(hf_dataset[data_idx]['labels'][hf_dataset[data_idx]['labels'].count(-100):], skip_special_tokens=True),  # hf_dataset[data_idx]['prompt'],  # only if using TEXT init\n",
    "            num_virtual_tokens=num_virtual_tokens,\n",
    "            tokenizer_name_or_path=MODEL_NAME, # pre-trained model name\n",
    "            num_transformer_submodules=1,\n",
    "            token_dim=foundational_model.model_dim  # // num_virtual_tokens\n",
    "        )\n",
    "        peft_model = get_peft_model(foundational_model, generation_config)\n",
    "        print(peft_model.print_trainable_parameters())\n",
    "        with torch.no_grad():\n",
    "            init_vtoken = peft_model.get_prompt(1)\n",
    "        \n",
    "        # Create directories to store the models\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.mkdir(working_dir)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.mkdir(output_directory)\n",
    "        \n",
    "        # Get training args\n",
    "        training_args = create_training_arguments(output_directory, learning_rate, epochs, device=device)\n",
    "        # Get trainer\n",
    "        trainer = create_trainer(peft_model, training_args, \n",
    "                                 train_dataset=hf_dataset.select(range(data_idx, data_idx+1)), \n",
    "                                 schedule_free=schedule_free,\n",
    "                                 unit_norm=unit_norm, unit_norm_scale_factor=unit_norm_scale_factor)\n",
    "        # Run training\n",
    "        trainer.train()\n",
    "        # Get trained model\n",
    "        peft_model = trainer.model\n",
    "        # Save if required\n",
    "        if save:\n",
    "            peft_model.save_pretrained(output_directory)\n",
    "\n",
    "    # Return virtual token\n",
    "    with torch.no_grad():\n",
    "        virtual_token = peft_model.get_prompt(1)\n",
    "    \n",
    "    return virtual_token, hf_dataset[data_idx]['prompt'], peft_model, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5d699d-dfd9-4163-b2b3-2312361d1376",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4b647f-f0d6-493d-96bb-144a97f882b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892bab372acf4a7d92d41778c7c2a13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global MODEL_NAME\n",
    "MODEL_NAME = \"t5-base\" # \"t5-base\"  # \"bigscience/bloomz-560m\" # \"bigscience/bloomz-560m\"  # \"gpt2\"\n",
    "\n",
    "# Load tokenizer    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if MODEL_NAME in ['gpt2', 'meta-llama/Llama-2-7b-hf']:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load dataset\n",
    "hf_dataset = get_tokenized_dataset(tokenizer, shuffle_input=False)\n",
    "\n",
    "# Load model\n",
    "if MODEL_NAME.startswith('t5'):\n",
    "    config = T5Config.from_pretrained(MODEL_NAME)\n",
    "    config.dropout_rate = 0\n",
    "    foundational_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, config=config, device_map='auto')\n",
    "else:\n",
    "    config = LlamaConfig.from_pretrained(self.kind)\n",
    "    config.attn_dropout = 0\n",
    "    foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if MODEL_NAME.startswith('meta') else torch.float32,\n",
    "        device_map='auto'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e163594-3d0d-4d2a-bca2-ef5be0e62732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the input_ids of the dataset\n",
    "# hf_dataset = get_tokenized_dataset(tokenizer, shuffle_input=True, seed=19, constant_input=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7a216-043a-4332-ac10-3c86d009c477",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0703952-d267-44ba-b520-6b01e0d1fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEAT_PATH=\"../cache/word2vec-2000/computer/computer_instruction_vtoken_t5-small_feats.bin\"\n",
    "features = torch.load(FEAT_PATH)\n",
    "warm_start_idxs = np.array([669, 1705, 814, 810, 1441]) - 1\n",
    "warm_start_features = features[warm_start_idxs]\n",
    "warm_start_norm_mean = torch.linalg.vector_norm(warm_start_features, dim=1).mean().item()\n",
    "full_set_norm_mean = torch.linalg.vector_norm(features, dim=1).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e940da1e-3987-4b3e-894f-8297b22c06f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_0 = features[0]\n",
    "feature_0_norm = feature_0.norm()\n",
    "feature_0_normalized = feature_0 / feature_0_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b540a3e-b2eb-4821-a67b-f5a1757cafed",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mix = features[1992]\n",
    "feature_mix_norm = feature_mix.norm()\n",
    "feature_mix_normalized = feature_mix / feature_mix_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47c6f1a-26d8-40cd-a3fc-3b8065505f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeds = foundational_model.get_input_embeddings()\n",
    "for p in token_embeds.parameters():\n",
    "    break\n",
    "# Based on prompt text\n",
    "_prompt = \"The task is to find a hidden test word by guessing new words. What is your next guess?\"\n",
    "prompt_embed = p[tokenizer(_prompt, return_tensors='pt')['input_ids'][0]][None, :, :]\n",
    "prompt_embed_norm_mean = prompt_embed.norm(dim=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee78a63e-bfdb-48d7-b168-b7fee2acda90",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 512 but got size 768 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rand \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(torch\u001b[38;5;241m.\u001b[39mrand(feature_0_normalized\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m vtoken \u001b[38;5;241m=\u001b[39m ((feature_0_normalized\u001b[38;5;241m.\u001b[39mcuda()) \u001b[38;5;241m*\u001b[39m prompt_embed_norm_mean)[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# * rand  # * warm_start_norm_mean\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m vtoken_plus_text \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((vtoken, prompt_embed), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# prepend vtoken to prompt embed\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Generate vtoken output\u001b[39;00m\n\u001b[1;32m      7\u001b[0m vtoken_output \u001b[38;5;241m=\u001b[39m get_outputs(foundational_model, inputs_embeds\u001b[38;5;241m=\u001b[39mvtoken_plus_text\u001b[38;5;241m.\u001b[39mtype(foundational_model\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m      8\u001b[0m                             device\u001b[38;5;241m=\u001b[39mdevice, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 512 but got size 768 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "for i in range(1, 1000, 1000):\n",
    "    rand = torch.nn.functional.normalize(torch.rand(feature_0_normalized.shape)*2-1, dim=0)\n",
    "    vtoken = ((feature_0_normalized.cuda()) * prompt_embed_norm_mean)[None, None, :].cuda()  # * rand  # * warm_start_norm_mean\n",
    "    vtoken_plus_text = torch.cat((vtoken, prompt_embed), dim=1)  # prepend vtoken to prompt embed\n",
    "    \n",
    "    # Generate vtoken output\n",
    "    vtoken_output = get_outputs(foundational_model, inputs_embeds=vtoken_plus_text.type(foundational_model.dtype),\n",
    "                                device=device, text=True)[0]\n",
    "    print(f'{vtoken_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0795773-5559-4513-89fb-b81fc6208a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9260fdb3-e3d5-4193-837d-d4561badd4a6",
   "metadata": {},
   "source": [
    "# Soft-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97f15276-f13c-4f9e-bbb7-6845a17bc63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 768 || all params: 222,904,320 || trainable%: 0.00034454244762954794\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super() argument 1 must be a type, not T5TokenizerFast",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get vtoken\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vtoken, prompt, peft_model, trainer \u001b[38;5;241m=\u001b[39m get_virtual_token(foundational_model, tokenizer, hf_dataset, \n\u001b[1;32m      3\u001b[0m                                                         data_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_virtual_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                                         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-2\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m MODEL_NAME\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m      5\u001b[0m                                                         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m, schedule_free\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, unit_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unit_norm_scale_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                                         save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[20], line 178\u001b[0m, in \u001b[0;36mget_virtual_token\u001b[0;34m(foundational_model, tokenizer, hf_dataset, data_idx, num_virtual_tokens, learning_rate, epochs, save, reset, schedule_free, unit_norm, unit_norm_scale_factor)\u001b[0m\n\u001b[1;32m    176\u001b[0m training_args \u001b[38;5;241m=\u001b[39m create_training_arguments(output_directory, learning_rate, epochs, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Get trainer\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m trainer \u001b[38;5;241m=\u001b[39m create_trainer(peft_model, training_args, \n\u001b[1;32m    179\u001b[0m                          train_dataset\u001b[38;5;241m=\u001b[39mhf_dataset\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(data_idx, data_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)), \n\u001b[1;32m    180\u001b[0m                          schedule_free\u001b[38;5;241m=\u001b[39mschedule_free,\n\u001b[1;32m    181\u001b[0m                          unit_norm\u001b[38;5;241m=\u001b[39munit_norm, unit_norm_scale_factor\u001b[38;5;241m=\u001b[39munit_norm_scale_factor)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m    183\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[20], line 115\u001b[0m, in \u001b[0;36mcreate_trainer\u001b[0;34m(peft_model, training_args, train_dataset, schedule_free, unit_norm, unit_norm_scale_factor)\u001b[0m\n\u001b[1;32m    112\u001b[0m     add_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (optimizer, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mpeft_model) \u001b[38;5;28;01mif\u001b[39;00m MODEL_NAME\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m DataCollatorWithPadding(tokenizer)  \u001b[38;5;66;03m# DataCollatorForLanguageModeling(tokenizer, mlm=False)\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m custom_collator \u001b[38;5;241m=\u001b[39m CustomCollator(tokenizer, model\u001b[38;5;241m=\u001b[39mpeft_model)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m    118\u001b[0m     _type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(eval_pred\u001b[38;5;241m.\u001b[39mpredictions) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m, in \u001b[0;36mCustomCollator.__init__\u001b[0;34m(self, tokenizer, model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer, model):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(tokenizer, model)\n",
      "\u001b[0;31mTypeError\u001b[0m: super() argument 1 must be a type, not T5TokenizerFast"
     ]
    }
   ],
   "source": [
    "# Get vtoken\n",
    "vtoken, prompt, peft_model, trainer = get_virtual_token(foundational_model, tokenizer, hf_dataset, \n",
    "                                                        data_idx=0, num_virtual_tokens=1,\n",
    "                                                        learning_rate=3e-2*(1 if MODEL_NAME.startswith('t5') else 1), \n",
    "                                                        epochs=40, schedule_free=False, unit_norm=True, unit_norm_scale_factor=1.,\n",
    "                                                        save=False, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1deba-ee60-4909-bd70-e19d50d31a58",
   "metadata": {},
   "source": [
    "### Generate vtoken output and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "921bd835-55f0-4a20-b5c1-1e669083dc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd\n",
      "\n",
      "Vtoken:\n",
      "-'. the at in hot, you your reception after for standing du as warm brief form back la to both and on an comfortable comfort per des over de welcomeen den dem second so!s un din bas der se par le further from... again no jao pre car this point im it long son of be short sweet counter all then ce: hotel am following underd right here rest summer last co/ 2 also key turn au pay finale up her out die (n sign con que two gentle swift more evening joint well stay soft\n"
     ]
    }
   ],
   "source": [
    "vtoken_output = get_outputs(foundational_model, inputs_embeds=vtoken.type(foundational_model.dtype), device=device, text=True, greedy=False)[0]\n",
    "print(f'Prompt:\\n{prompt}\\n')\n",
    "print(f'Vtoken:\\n{vtoken_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717cdbe5-0abb-438a-be47-a72f6ebd16ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Perturb vtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bd22fc9-91f3-4f58-8fb9-5bc9c72dd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(vtoken.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1fd8f2c-f33c-43bb-bcde-20c425934ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(vtoken.shape)\n",
    "mask[:, :, :75] = 1\n",
    "mask = mask[:, :, torch.randperm(mask.nelement())]\n",
    "new_vtoken = (vtoken + (rand*mask).to(device) * 1000) * 1.\n",
    "# get_outputs(foundational_model, \n",
    "#             inputs_embeds=new_vtoken, \n",
    "#             # decoder_inputs_embeds=decoder_inputs_embeds[:, :-10, :]*100,\n",
    "#             device=device, text=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24146d4-921d-4569-bc1b-f4671eb3a64a",
   "metadata": {},
   "source": [
    "### Combine vtoken and textual prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71c8f04e-aff9-4f35-8356-af695a5c3663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 103, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeds = foundational_model.get_input_embeddings()\n",
    "for p in token_embeds.parameters():\n",
    "    break\n",
    "# Based on prompt text\n",
    "# _prompt = \" \".join(prompt.split()[:-1])  # Everything except pwd/chmod\n",
    "# prompt_embed = p[tokenizer(_prompt, return_tensors='pt')['input_ids'][0]][None, :, :]\n",
    "\n",
    "# Based on prompt input_ids\n",
    "if MODEL_NAME.startswith('t5'):\n",
    "    prompt_input_ids = hf_dataset[0]['input_ids']  # encoder input\n",
    "else:\n",
    "    prompt_input_ids = hf_dataset[0]['input_ids'][:hf_dataset[0]['labels'].count(-100)]  # inputs for which no predictions need to be made\n",
    "_prompt = tokenizer.decode(prompt_input_ids, skip_special_tokens=True)\n",
    "prompt_embed = p[prompt_input_ids][None, :, :]\n",
    "\n",
    "# vtoken_norm_and_scaled = torch.nn.functional.normalize(vtoken)*prompt_embed.norm(dim=-1).mean()\n",
    "vtoken_plus_text = torch.cat((vtoken, prompt_embed), dim=1)  # prepend vtoken to prompt embed\n",
    "vtoken_plus_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5bfd9ec1-1689-4478-9db9-1c09bb940701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets like this. my first command is\n",
      "\n",
      "Vtoken:\n",
      "pwd: I want you to act as a linux terminal. type commands and you will reply with what the terminal should show. do not write explanations. do not type commands unless I instruct you to do so. my first command is like this, so i will type it in curly brackets like this. my second command is like this, so i will type it into curly brackets like this. my last command is like this\n"
     ]
    }
   ],
   "source": [
    "# Generate vtoken output and compare\n",
    "vtoken_output = get_outputs(foundational_model, inputs_embeds=vtoken_plus_text.type(foundational_model.dtype),\n",
    "                            device=device, text=True, greedy=False)[0]\n",
    "print(f'Prompt:\\n{_prompt}\\n')\n",
    "print(f'Vtoken:\\n{vtoken_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a145bc0-7f16-4c13-8e54-813c81d045c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(348.1645, device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embed.norm(dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ffb7f05-8eb1-49c3-8265-2956f73061a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(96.4136, device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embed.norm(dim=-1).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4dcb903e-e75b-4eb3-adc3-4e2e7ab15efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(228.3652, device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embed.norm(dim=-1).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7ae0111-e8f9-46ef-a862-af46ff30735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(532.9974, device='cuda:0')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embed.norm(dim=-1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1339c74-7117-42a6-a821-835ea570757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0') tensor(1.6794e-05, device='cuda:0') tensor(0.0361, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(vtoken.norm(), vtoken.mean(), vtoken.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1721755-d936-48e4-996f-2e25afb157d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Comparing different vtokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f88c1c9-7725-4961-b59d-cebd4e201f18",
   "metadata": {},
   "source": [
    "# Shuffled Prompt (seed=17)\n",
    " tell commands code not unique should inlike inside and to with. command the need, not reply english linux will I what want nothing you terminal block you as putting unless reply so first. else do when so terminal want you show onely} do to write.I this to, { commands you and do by you instruct is a text something i to. act explanations I will I. terminal inside with type the brackets i. type only my cur do output will"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8a15a6a-5ad1-40fc-ad35-74b99b9c8775",
   "metadata": {},
   "source": [
    "# Shuffled Prompt (seed=18)\n",
    " show youlike you nothing reply I when cur i is. unique to terminal this the command and tell should} terminal english by not need you brackets commands do what willI in to and. i instruct output, inside one putting reply commands code so inside linux with something explanations so. with I type first you act unless do do, to do a want terminal { not the. you. to will I only want type else as my will. text block writely\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb06f8ab-f0cb-4757-8b51-3fc3d1457c13",
   "metadata": {},
   "source": [
    "# Shuffled Prompt (seed=19)\n",
    "} as tell you to I do. you unique do with by terminal commands text type and output puttingly block you is. something and to. I not act not a want want need brackets. you explanations first to nothing what one the I, you. will commands when i. type terminal else with code reply should,I i my write to will this only terminal will unless command so show instruct english linux do cur reply so inside do in thelike { inside\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a39e5d29-f80b-4088-959e-88b57f82e953",
   "metadata": {},
   "source": [
    "# Constant Prompt (0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ea6da6-44b7-4029-8d87-8f2ffe314e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtoken1 = vtoken  # Unshuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22d3809b-7641-4a24-ac11-9630e3434cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtoken2 = vtoken  # Shuffled (seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3849f7cf-5f31-46ca-b94f-4c15f12e921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtoken3 = vtoken  # Shuffled (seed=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dc9cd2d-cd3c-4704-84f8-72e5c67d0848",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtoken4 = vtoken  # Shuffled (seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e87d9ef0-642f-4e28-9dfc-ee3523fc4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "vtoken5 = vtoken  # Constant (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2902ed2a-9955-4409-a465-9a21f940f1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[925.2473]]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken1, vtoken2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93384cfc-4e60-47c8-b28b-4b5c4aa04d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[867.6586]]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken1, vtoken3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a89214b-0466-4e62-8d66-d65923f79b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1069.6093]]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken1, vtoken4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f3edd69-ed91-4016-b337-a944f5105119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1095.5792]]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken1, vtoken5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2f1d85a-6fdb-469a-a82f-2367237ebcd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[427.0382]]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken2, vtoken3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1fb0974-65d3-4d9f-a286-d8acaaf190da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[670.3230]]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken2, vtoken4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "848349d7-0e1f-49ce-a159-7f65df9643db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[888.1259]]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken2, vtoken5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac8c398d-6726-4be4-bc5d-59582d757c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[678.1097]]], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken3, vtoken4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9e1cd18c-9ac5-458c-a4a2-a827fe13dee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[891.1073]]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken3, vtoken5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5e8d1423-f4f8-4fe7-b8e4-eedcb5bb26d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1059.0133]]], device='cuda:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(vtoken4, vtoken5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc57046-484c-434c-9ec3-a3fab64f8ebd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Check output with peft model using textual prompt (t5-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4b39f-83c5-465c-a50c-a71e07bd0943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR T5 models only\n",
    "input_tokenized = tokenizer(\"Instruction\", return_tensors='pt')\n",
    "input_tokenized = {**input_tokenized, \"attention_mask\": input_tokenized[\"attention_mask\"]*0}\n",
    "vtoken_output = get_outputs(peft_model, inputs=input_tokenized, device=device, text=True)[0]\n",
    "print(f'Prompt:\\n{prompt}\\n')\n",
    "print(f'Vtoken:\\n{vtoken_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
